![](https://cloud.overment.com/S04E01-1732441016.png)

W przykładach dołączonych do dotychczasowych lekcji często łączyliśmy model z zewnętrznymi narzędziami. Zazwyczaj były to proste interakcje, które pozwalały na dostarczanie zewnętrznego kontekstu lub wykonywanie nieskomplikowanych zadań. Co więcej, sposób korzystania z tych narzędzi, tylko w małym stopniu leżał po stronie LLM i w większości opisywaliśmy go z pomocą kodu, tworząc tzw. chain.

Teraz przeniesiemy to na wyższy poziom, przygotowując narzędzia, które przesuną więcej odpowiedzialności na model językowy. Jednocześnie zachowamy możliwie dużą kontrolę oraz wsparcie dla modelu po stronie kodu. Inaczej mówiąc: 

- LLM nie będzie odpowiedzialny tylko za generowanie danych wejściowych i sterowanie parametrami funkcji
- W zamian LLM otrzyma narzędzie (zestaw funkcji), instrukcję obsługi oraz komplet informacji umożliwiających niemal całkowicie autonomiczne działanie

Pierwsze przykłady integracji modeli językowych (LLM) z zewnętrznymi narzędziami przedstawiliśmy w lekcji S01E01. Teraz zgłębimy ten temat bardziej kompleksowo, zwiększając precyzję działania i odporność na potencjalne błędy.

W ramach wstępu zaznaczę także obecność Function Calling (lub Tool Use) - funkcjonalności oferowanych przez OpenAI i Anthropic. Ich założeniem jest ułatwienie interakcji z modelem, gdy pojawiają się informacje z zewnętrznych źródeł. Celowo jednak ograniczę ich zastosowanie, ponieważ narzucają szereg ograniczeń, nie oferując w zamian wartości uzasadniającej ich wybór. Zaznaczę tylko, że koncepcje, które będziemy omawiać, będą miały zastosowanie także w przypadku tych funkcjonalności, więc nie będzie przeszkód, aby po nie sięgnąć.

W zamian wykorzystamy techniki prowadzenia interakcji z modelem, o których dotychczas mówiliśmy, wspierając się przy tym JSON Mode i Structured Output. Główną zaletą tego podejścia jest elastyczność, umożliwiająca nawet przełączanie się między modelami różnych dostawców. Ma to szczególne znaczenie ze względu na dynamiczny rozwój branży i potencjalną potrzebę szybkiego przechodzenia między modelami.
## Założenia narzędzi dla agentów

Przez "narzędzie" dla agenta AI rozumiemy zestaw funkcji i promptów. Umożliwiają one LLM (zwykle) autonomiczne korzystanie z zewnętrznych usług, serwisów, aplikacji czy urządzeń. Obejmuje to zarówno zarządzanie listą zadań w Linear, jak i wykonywanie działań na urządzeniach połączonych z Internetem. Narzędziem może być także inna generatywna aplikacja, wyspecjalizowana w określonym zadaniu — inaczej mówiąc: inny agent AI.

Do tej pory wielokrotnie przekonaliśmy się, że przewidywalność działania modeli jest ograniczona. Nigdy nie mamy pewności, czy otrzymamy oczekiwany rezultat. Zamiast tego poruszamy się w obszarze **prawdopodobieństwa**, dążąc do zwiększania szansy na działanie zgodnie z założeniami.

Wyraźnie sugeruje to, że **narzędzia dla agentów nie powinny realizować krytycznych procesów**, ich kontakt ze światem zewnętrznym powinien być możliwie ograniczony, a podejmowane akcje odwracalne. Z drugiej strony otwieramy przed sobą przestrzeń do realizowania procesów, które do tej pory pozostawały poza zasięgiem kodu. 

Narzędzie dla LLM musi zatem spełniać następujące wymagania: 

- Jego nazwa musi być oczywista, zwięzła, unikatowa i charakterystyczna, aby model mógł z wysoką pewnością stwierdzić, które narzędzie wybrać spośród dostępnej listy umiejętności
- Każde narzędzie powinno zawierać **zwięzły opis**. Taki opis nie tylko ułatwi modelowi wybór odpowiedniego narzędzia, ale także przedstawi jego możliwości i ograniczenia
- Narzędzie musi posiadać także instrukcję obsługi, która może mieć formę jednego lub wielu promptów. Wynikiem ich działania będzie obiekt JSON/YAML zawierający właściwości potrzebne do uruchomienia funkcji
- Narzędzie musi mieć zdefiniowaną **strukturę danych wejściowych**, **zależności**, **strukturę danych wyjściowych** oraz **listę dostępnych akcji**. Inaczej mówiąc, każde z narzędzi musi być zbudowane tak, aby możliwe było korzystanie z nich **w różnych konfiguracjach**. 

Dodatkowo musimy zadbać o aspekty związane z logiką używania narzędzi w skład której wchodzi między innymi:

- Zestawy danych testowych oraz testy automatyczne dla wszystkich promptów
- Zapisywanie historii podjętych działań, pozwalających na wczytanie zapytania i wyniku dla 
- Obsługa błędów z opcją automatycznego naprawienia
- System wykonywania akcji asynchronicznych (kolejka, reagowanie na zdarzenia lub działanie według harmonogramu)

Zatem koncepcja 'narzędzia' kształtuje nam się na formę niezależnie działającego modułu aplikacji, którym może posługiwać się LLM. Spróbujmy więc to wszystko zebrać w całość.

## Struktura interfejsu narzędzi

Zacznijmy od zarysowania ogólnej perspektywy tego, jaki zakres możliwości mogą oferować narzędzia dla agentów. Kilka przykładów: 

- Zarządzanie listą zadań, nawet w kontekście całego projektu
- Zarządzanie kalendarzami i komunikacją w związku z nimi (szkice maili, podsumowania, raporty)
- Transformacja obszernych dokumentów (np. tłumaczenie)
- Generowanie rozbudowanych form (np. test, audio, obraz)
- Zaawansowane przeszukiwanie Internetu
- Zarządzanie harmonogramem zadań cyklicznych
- Systemy powiadomień i wiadomości (Slack, E-mail, SMS)
- ...i inne

Narzędzia te będą mogły być uruchamiane przez model indywidualnie, ale także w połączeniu. W rezultacie agent będzie mógł otrzymać polecenie takie jak: "Codziennie rano wejdź na strony `...`, napisz streszczenie ich treści i wyślij mi je na e-mail", na podstawie którego ustali plan akcji i wykona je.

Spróbujmy więc zbudować narzędzie pierwsze z listy, które kompleksowo zaopiekuje się naszą listą zadań i będzie to robić w sposób na tyle dobry, że będziemy chcieli z niego korzystać na co dzień. Co więcej, wypracowany schemat powinien otworzyć nam drogę do budowania kolejnych narzędzi, które ostatecznie będą ze sobą współpracować. Dla uproszczenia będziemy posługiwać się narzędziem [Todoist](https://todoist.com/) ze względu na jego popularność, ale ustalimy interfejs pozwalający na możliwie łatwe przełączenie się na inne rozwiązanie. 

Akcje narzędzia do zarządzania aplikacją to-do, uwzględniają: 

- Pobieranie listy projektów
- Pobieranie listy zadań
- Dodawanie, modyfikowanie oraz usuwanie zadań
- Obserwowanie listy zadań

Natomiast sam interfejs narzędzia prezentuje się tak: 

- **Input:** Lista wiadomości z konwersacji, opcjonalny kontekst w postaci np. dokumentu, notatki ze spotkania, opisu zdjęcia czy aktualnej lokalizacji
- **Output:** Lista XML zawierająca szczegóły podjętych działań, wraz z ich statusem oraz informacją zwrotną

A poszczególne akcje tak: 

- **Lista projektów** 
	- Input: brak
	- Output: tablica obiektów z ID, nazwą, opisem oraz liczbą aktywnych zadań **lub informacja o błędzie**
- **Lista zadań (list):**
	- Input: ID projektów, ID zadań, zakres dat, status
	- Output: tablica obiektów z ID, ID projektu, nazwą, opisem, statusem, datą rozpoczęcia, datą aktualizacji **lub informacja o błędzie**
- **Zadanie (get):**
	- Input: ID zadania
	- Output: obiekt z ID, ID projektu, nazwą, opisem, statusem, datą rozpoczęcia, datą aktualizacji **lub informacja o błędzie**
- **Add Task (add):** 
	- Input: ID projektu, nazwa, opis, status, data rozpoczęcia
	- Output: obiekt z ID, ID projektu, nazwą, opisem, statusem, datą rozpoczęcia, datą aktualizacji **lub informacja o błędzie**
- **Update Task (update):**
	- Input: ID zadania, przynajmniej jedno z opcjonalnych pól zadania
	- Output: obiekt z ID, ID projektu, nazwą, opisem, statusem, datą rozpoczęcia, datą aktualizacji **lub informacja o błędzie**
- **Understand Query:**
	- Input: oryginalne zapytanie użytkownika
	- Output: obiekt zawierający zapytania typu "add", "update", "delete", "list", "get". 

Także w przypadku tych akcji, chodzi nam o zapewnienie **kompletu informacji** wejściowych potrzebnych do podjęcia działania oraz danych wyjściowych, potrzebnych LLM do określenia tego, co należy zrobić dalej. 

## Instrukcje dla modelu

Teraz potrzebujemy serii promptów dla modelu, wraz z zestawem danych testowych oraz testów automatycznych, które pozwolą nam na uzyskanie możliwie wysokiego poziomu skuteczności.

Zacznijmy od zdefiniowania nazwy, oraz opisu narzędzia, które budujemy:

- **nazwa:** manage_todo
- **description**: Use this tool to access & manage Todoist tasks and projects. Available actions: **listing projects, retrieving tasks, adding, modifying, and deleting tasks, and monitoring for task updates.**

Nie ma więc tutaj miejsca na wątpliwości, co do tego, czym jest to narzędzie, oraz jakie akcje są dostępne. 

Pierwsza akcja, czyli `Lista Projektów` nie wymaga żadnych danych wejściowych, więc nie wymaga promptu. Natomiast zwrócony przez nią rezultat będzie mieć formę obiektu JSON, który będziemy mapować do tekstu z tagami `XML-like`.

Druga akcja, czyli `Lista zadań` wymaga już instrukcji dla modelu, ponieważ musimy mieć możliwość zawężenia wyszukiwania względem daty oraz wybranych projektów. Treść takiego promptu znajduje się w pliku `todo/prompts/list_tasks.ts`, a on sam został wygenerowany z użyciem meta promptu, który omawialiśmy w lekcji S00E02 — Prompt Engineering i kilku iteracji kształtujących jego zachowanie. 

![](https://cloud.overment.com/2024-10-15/aidevs3_list_tasks-40bd7275-c.png)

Poza instrukcją, w pliku znajduje się także zestaw przykładowych danych, oraz seria testów weryfikujących działanie promptu. Test można uruchomić poleceniem `bun todo/prompts/list_tasks.ts`, co po kilku/kilkunastu sekundach zwróci rezultat w postaci tabeli z wynikami PromptFoo. Same testy również zostały wygenerowane przez LLM pod moim okiem (aczkolwiek mogą pojawić się tam pojedyncze nieścisłości). 

![](https://cloud.overment.com/2024-10-15/aidevs3_list_tasks_test-d08b7d63-4.png)

Wszystkie pozostałe prompty są przygotowane mniej więcej według tych samych zasad. Konkretnie:

- Struktura promptu podąża schematem: "Rola | Cel | Format odpowiedzi | Zasady | Przykłady | Wezwanie do akcji."
- Prompty zawierają **minimum niezbędnego kontekstu** do działania i skupiają się na **jednym kroku.**
- Prompty generują obiekty JSON, których pierwsza właściwość to zawsze "`_thinking`," aby wydłużyć czas wypowiedzi modelu.
- Prompty zawierają kontekst w postaci **listy projektów** oraz (część z nich) **listy zadań.** Dodatkowo, tam gdzie to możliwe, daty umieszczone w prompcie generowane są **dynamicznie,** ponieważ z nimi LLM ma najwięcej problemu.
- Zestawy danych testowych mają na celu sprawdzenie typowych problemów z działaniem promptu i, tam gdzie to możliwe, weryfikację wypowiedzi modelu za pomocą kodu (JavaScript), szukając słów kluczowych.
- Tylko do dwóch promptów trafiają oryginalne wiadomości użytkownika. Pozostałe pracują na zapytaniach wygenerowanych przez model w celu zmniejszenia zakresu danych, które muszą być brane pod uwagę.

Kolejny prompt `get_task.ts` jest dość opcjonalny, ponieważ w tym przypadku nie pracujemy z dodatkowymi informacjami na temat zadania, takimi jak komentarze, etykiety czy pod-zadania. Dołączam go jednak w celach poglądowych.

Ostatnie z promptów odpowiedzialnych za akcje: `add_tasks` i `update_tasks` mają jedną bardzo istotną cechę — umożliwiają **przetwarzanie wielu rekordów jednocześnie**. Dzięki temu model może zdecydować, na przykład, o rozbiciu zapytania użytkownika na kilka zadań lub wymianie informacji między różnymi rekordami. Ten detal sprawia, że z perspektywy użytkownika większą wartość ma przesłanie jednej wiadomości opisującej serię zadań, niż wpisywanie ich ręcznie. Gdyby model pracował na indywidualnych rekordach, w większości przypadków wygodniejsze byłoby dodanie zadań samodzielnie.
\
No i na samym końcu mamy prompt, który można określić jako 'mózg' naszego narzędzia. Odpowiada on za **interpretację rozmowy z użytkownikiem**, stworzenie planu działań oraz wygenerowanie zapytań dla każdej z akcji. Efekt jego działania widzimy poniżej. Zawiera on refleksję na temat tego, co powiedział użytkownik, oraz opis akcji (w tym przypadku aktualizacji zadania, które było już na liście). 

![](https://cloud.overment.com/2024-10-15/aidevs3_tool_plan-634e98fe-0.png)

Ostatecznie, struktura całego narzędzia prezentuje się następująco: 

- Zapytanie użytkownika zostaje odebrane przez serwer
- Model planuje działania związane z zapytaniem użytkownika
- Następnie dochodzi do **równoległego** wykonania zapytań i zgromadzenia odpowiedzi (lub pominięcia tego kroku w przypadku, gdy rozmowa nie dotyczy zarządzania listą zadań)
- Ostatnim krokiem jest wygenerowanie odpowiedzi i zwrócenie jej użytkownikowi

![](https://cloud.overment.com/2024-10-15/aidevs3_tasks_tool-0e187ff2-a.png)

Zanim przejdziemy dalej, warto przyjrzeć się treści samych promptów, prezentowanych w nich przykładach oraz kategoriach projektów, a sama lista zadań zwykle nie przekracza kilkunastu rekordów. Wszystkie te dane są dopasowane **do mojego stylu pracy** i z dużym prawdopodobieństwem, nie sprawdzą się w każdej sytuacji.

Bez względu na preferencje, istotne jest tutaj **dopasowanie tych informacji do samego modelu**, kierując się wskazówkami, które omawialiśmy wcześniej. Chociażby sama lista projektów zawiera **nazwy i opisy**, pozwalające łatwo dopasować zadanie. 

![](https://cloud.overment.com/2024-10-15/aidevs3_projects-ef92f948-b.png)

Może się jednak okazać, że powyższe opisy **nie będą wystarczające** i zajdzie potrzeba dodania **ogólnego kontekstu** zawierającego informacje na nasz temat, co również przyczyni się do zwiększenia skuteczności dopasowania. Przykład: **Ukończenie lekcji kursu AI_devs 3** to z mojej perspektywy zadanie dla kategorii "Act", a nie "Learn". Z kolei z Twojej perspektywy, będzie odwrotnie.

Po raz kolejny przekonujemy się zatem, że **kontekst ma znaczenie**.
## Struktura interfejsu danych i obsługa błędów

W klasycznych aplikacjach występują dwie główne kategorie błędów: nieprzewidziane (np. brak dostępu do usługi) oraz te spowodowane działaniami użytkownika (np. niepoprawne hasło). W takim przypadku użytkownik otrzymuje informację o możliwych dalszych krokach. Informacja ta może być dość ogólna, ale mimo to użytkownik jest w stanie podjąć dalsze działania, w tym skontaktować się z obsługą techniczną.

W przypadku LLM jest nieco inaczej, ponieważ poza błędem, zwykle będziemy potrzebowali przekazać także kontekst opisujący ewentualne dalsze kroki. Dodatkowo reakcja na takie scenariusze, musi być opisana w kodzie. 

Poniżej mamy sytuację, w której proszę o wprowadzenie zmian w zadaniu, **które nie istnieje**. Pomimo tego, asystent poprawnie odnajduje się w tej sytuacji i sugeruje dalsze kroki. 

![](https://cloud.overment.com/2024-10-15/aidevs3_error-3493ff70-6.png)

Z drugiej strony, istnieją sytuacje w których nasz asystent nie odnajduje się w pełni ze względu na ograniczony dostęp do informacji. Poniższa prośba o 'przesunięcie wszystkich zadań do projektu X', została co prawda zrealizowana poprawnie, ale komunikat o jej realizacji jest błędny. Wynika on z tego, że w kontekście znajdują się **zaktualizowane już zadania** i nie ma śladu po tym, że zostały wykonane na nich jakieś akcje. 

![](https://cloud.overment.com/2024-10-15/aidevs3_context-9278686b-6.png)

Już na etapie planowania narzędzia musimy uwzględnić przepływ informacji między promptami, aby uniknąć rozbieżności między tym, co się wydarzyło, a tym, o czym wie asystent. Dla porównania, widzimy, że dodanie prostej informacji na temat "ID poprzedniego projektu" wystarczyło, aby asystent poprawnie zorientował się w sytuacji i właściwie odpowiedział. 

![](https://cloud.overment.com/2024-10-15/aidevs3_moved-ce8507b9-0.png)

To wszystko prowadzi nas do następujących obserwacji i wniosków: 

- Narzędzie (tool) przyjmuje **polecenie w naturalnym języku** oraz w taki sam sposób odpowiada.
- Akcje zaplanowane są tak, aby **móc odrzucić zapytania, które nie są zgodne z wytycznymi**. 
- Akcje **są stosunkowo elastyczne** i nie opierają się o określone polecenia, czy słowa kluczowe.
- Najbardziej wrażliwe elementy akcji są **sterowane programistycznie**
- Narzędzie (tool) może działać **niezależnie**. Oznacza to, że może skorzystać z niego człowiek, działająca w tle automatyzacja, bądź agent AI
## API asystenta i proxy dostępu do zewnętrznych usług

Przykład `todo` oferuje integrację z Todoist, jednak model **nie komunikuje się bezpośrednio z jego API**, lecz posługuje się zestawem funkcji zdefiniowanych w `TaskService.ts`. Jest to rzecz, o której wspominałem już wcześniej, jednak teraz widzimy ją w praktyce. 

Nawet wspomniany powyżej przykład "przenoszenia projektów" i dodatkowego kontekstu w postaci "ID poprzedniego projektu" pokazuje, że zwykle będzie zależało nam na **dopasowaniu interakcji z API** do modelu.

Podkreślam tę kwestię, ponieważ spotkałem się z przykładami integracji łączących się z API, bez dodatkowych warstw po stronie aplikacji. Jest to nie tylko ograniczające, ale także stanowi wyzwanie z punktu widzenia bezpieczeństwa. 
## Zależności i relacje pomiędzy narzędziami

Wspominałem na początku lekcji, że narzędzia takie jak przykład `todo`, będą mogły ze sobą współpracować. Teraz na kilku przykładach przedstawię, co miałem na myśli (o kolejnych dowiemy się w nadchodzących lekcjach).

Przede wszystkim, nawet zrzuty ekranu z tej lekcji pokazują, że przykład `todo` połączyłem z interfejsem w postaci aplikacji Alice. Natomiast rolę takiego interfejsu mogą pełnić przeróżne usługi, aplikacje i skrypty.

Przede wszystkim, korzystając z wiedzy z lekcji S01E05 — Produkcja, skonfigurowałem serwer VPS, dzięki czemu przykład `todo` jest dla mnie dostępny zdalnie. Mogę więc utworzyć scenariusz automatyzacji na platformie [make.com](https://www.make.com/) i z pomocą dosłownie kilku kliknięć zbudować mechanizm obserwujący wybrane etykiety w Gmail. Wszystkie maile, które je otrzymają, zostaną dodane do mojej listy, a LLM zdecyduje o tym, jak je opisać i do którego projektu przyporządkować. 

![](https://cloud.overment.com/2024-10-15/aidevs3_make-7f6b9583-2.png)

Analogiczne automatyzacje mogę stworzyć dla innych narzędzi: komunikatorów, kanałów RSS czy filmów na YouTube. Pozwala to więc na podłączenie wielu źródeł, w przypadku których wystarczy przesłanie ich treści. Co więcej, **poza treścią, możemy przesłać także instrukcję z prośbą o ZDECYDOWANIE** o tym, czy dany wpis w ogóle powinien trafić na naszą listę zadań czy listę nauki. 

Połączenie z przykładem `todo` może odbywać się także poprzez aplikację Siri Shortcuts w ekosystemie Apple. Utworzone makro może nagrać wiadomość głosową, a następnie jej treść przesłać jako treść zadań. W zależności od potrzeb, możemy połączyć się tam także z modelem Whisper czy modelem przetwarzającym zrobione przez nas zdjęcie. 

![](https://cloud.overment.com/2024-10-15/aidevs3_shortcuts-3afa848f-6.png)

Takie integracje nie muszą mieć wyłącznie formy "zapisu", ale także odczytu. W końcu nic nie stoi na przeszkodzie, aby automatyzacja pobierała zadania, które mamy na liście i przygotowywała nam notatki mogące pomóc w ich wykonaniu. Np. dodajemy zadanie o naprawieniu błędu o określonej treści, a w opisie zadania pojawia się sugestia jego rozwiązania. 
## Podsumowanie

Patrząc na przykład `todo` dość dobrze widać po nim, że może stanowić niezależnie działającą aplikację, z którą można komunikować się z pomocą naturalnego języka. Dzięki automatycznym testom promptów (wskazane byłoby także przetestowanie samego kodu), możemy z stosunkowo dużą swobodą wprowadzać zmiany w logice. 

W ten sposób możemy 'otoczyć się' takimi modułami, które docelowo będą mogły komunikować się ze sobą i wykonywać dla nas (lub dla siebie nawzajem) przeróżne zadania. 

No i właśnie na tym polega koncepcja 'narzędzi dla agentów AI'.

Jeśli z tej lekcji chcesz wynieść tylko jedną rzecz, będzie to wymagało od Ciebie nieco więcej uwagi. Chodzi o umiejętność tworzenia niezależnych narzędzi, w których dopracowane prompty umożliwiają działanie z bardzo niskim poziomem błędów lub nawet ze 100% skutecznością. 

Narzędzia, które w ten sposób stworzysz, nie muszą być szczególnie rozbudowane. Wystarczy, że będą dobrze robić nawet jedną, małą rzecz. Małe rzeczy mają to do siebie, że ich wartość szybko się sumuje.

Powodzenia!
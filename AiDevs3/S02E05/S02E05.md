![](https://cloud.overment.com/2024-11-14/s02e05-9a2a5422-c.png)

W poprzedniej lekcji S02E04 — Połączenie wielu formatów zobaczyliśmy wartość wynikającą z jednoczesnego zastosowania wielu modeli, szczególnie w kontekście multimodalności. Natomiast teraz, przyjrzymy się kilku przykładom przełożenia tej wartości na praktyczne, codzienne zastosowania. 

Naszym celem nie jest jedynie zbudowanie kilku kolejnych narzędzi, ale spojrzenie z szerszej perspektywy na schemat ich powstawania. To pozwoli nam przenosić je na inne obszary i rozwiązywać różne problemy. Przekonamy się także, że niektóre, nierzadko bardzo użyteczne produkty AI, są bardzo proste do odwzorowania.
## Strukturyzowanie tekstu na potrzeby audio

Przynajmniej kilkukrotnie wspominałem, że LLM znacznie lepiej radzą sobie z transformacją treści niż jej generowaniem. Może to sugerować, że zadanie takie jak zamiana artykułu na formę audio dzięki Text to Speech powinno być prostym zadaniem. Okazuje się jednak, że nie do końca tak jest, ponieważ musimy uwzględnić przynajmniej kilka istotnych elementów.

Przede wszystkim w zależności od wykorzystanych modeli, będą obowiązywać nas limity zarówno dla danych wejściowych jak i wyjściowych, którym bliżej przyjrzymy się w trzecim tygodniu AI_devs. 

![](https://cloud.overment.com/2024-10-04/aidevs3_limits-a467bac7-5.png)

Dodatkowym problemem będą obrazy i zewnętrzne odnośniki, które wymagają wygenerowanych opisów, o czym mieliśmy okazję przekonać się w przykładzie `captions` i pracy z VLM. 

Ostatecznie wyzwaniem jest także odpowiednie stworzenie promptu, który poradzi sobie z dopasowaniem stylu wypowiedzi modelu tak, abyśmy nie skończyli z "fascynującym i ekscytującym doświadczeniem" (są to frazy często stosowane przez model przy transformacji treści) ani odnoszeniem się do "poniższego obrazka", który trudno jest zobaczyć na nagraniu.

![](https://cloud.overment.com/2024-10-04/aidevs3_graphic-76e25e0e-8.png)

W przykładzie `read` znajduje się kod zdolny do zamiany treści naszych lekcji, na przystępną do przesłuchania formę audio, z zachowaniem oryginalnego stylu wypowiedzi. W podkatalogach `demo` oraz `demo_2` znajdują się wygenerowane z pomocą ElevenLabs nagrania. Jego logika podzielona jest w tym przypadku na trzy kroki: mapowanie stylu, przekonwertowanie oryginalnego artykułu oraz wygenerowanie pliku audio (dla uproszczenia pominąłem logikę przykładu `caption` do opisywania obrazów).

![](https://cloud.overment.com/2024-10-04/aidevs3_toaudio-dd496dac-6.png)

Kluczową rolę w takich transformacjach odgrywa prompt, który nie powinien wskazywać na generowanie nowej treści na podstawie obecnej (np. "wygeneruj podcast na podstawie poniższego dokumentu"), lecz na jej **transformację** według określonych zasad. 

![](https://cloud.overment.com/2024-10-04/aidevs3_toaudioprompt-f83f6732-4.png)

W tym wszystkim chcemy uwzględnić jeszcze jeden, bardzo istotny detal, związany z tłumaczeniem na język polski — odmiany słów. Przykład typowego dla modeli błędu widzimy poniżej i jest on zwykle związany z próbą przetłumaczenia wyrażeń z języka angielskiego, bez zadbania o ich odpowiednią formę. 

![](https://cloud.overment.com/2024-10-04/aidevs3_translation-f8251ba5-1.png)

Tutaj również możemy w dużym stopniu to naprawić, wprowadzając zmiany w oryginalnej instrukcji, które podkreślą konieczność dbania o właściwe formy, lub poprzez dodatkowy prompt, zajmujący się wyłącznie korektą tłumaczenia.
## Rozmowa z mapą myśli

W lekcji S02E04 — Połączenie wielu formatów wspomniałem o przykładzie "rozmowy z mapą myśli", które podstawowa implementacja znajduje się w katalogu `mindmap`. Pozwala to nam na zamianę notatki głosowej na mapę myśli widoczną poniżej. Pomimo tego, że nagranie było bardzo chaotyczne i momentami trudne do zrozumienia, model poprawnie rozpoznał wszystkie wymienione koncepcje i przełożył je na wizualizację. 

![](https://cloud.overment.com/2024-10-04/aidevs3_mindmap-f4a7a7e9-7.png)

Sama logika przykładu jest bardzo prosta i nie zawiera w sobie żadnych nowych rzeczy, poza zastosowaniem `markmap-cli` do wygenerowania mapy myśli w formie Markdown i HTML. Jak zwykle nic nie stoi na przeszkodzie, aby przetwarzanie było bardziej rozbudowane i uwzględniało nawet połączenie z Internetem w celu wzbogacania naszej wizualizacji. 

![](https://cloud.overment.com/2024-10-04/aidevs3_mind-e386986b-8.png)

Alternatywnie moglibyśmy skorzystać z interfejsu przykładu `audio` (nową wersję umieściłem w katalogu `audio-map` oraz `audio-map-frontend`), aby "porozmawiać z mapą myśli" dokładnie tak, jak widzimy to na poniższym filmie. 

<div style="padding:75% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1016116728?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="map-concise"></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

Powyższy film został nieco przyspieszony, natomiast czas reakcji i tak jest dość niski. Mamy także przestrzeń do optymalizacji poprzez skorzystanie z Whisper Turbo oraz ElevenLabs Turbo. Ewentualną korektę mogą przejść jeszcze prompty odpowiedzialne za budowanie mapy myśli, co w przypadku zaawansowanej implementacji moglibyśmy uwzględnić jakiś rodzaj mechanizmu "diff". Wówczas nie byłoby potrzeby aktualizowania całej mapy, lecz tylko jej fragmentów.  

Logikę "voice to mind map" można przełożyć na interfejs działający w czasie rzeczywistym, co jest możliwe szczególnie z zastosowaniem platform takich jak Groq, które charakteryzuje szybka inferencja. Dobrym przykładem jest [wpis z profilu](https://x.com/Mlearning_ai/status/1816065857228767474) `@Mlearning_ai`, który przedstawia nagranie z rozmowy z strukturą tabeli prezentującej plan wycieczki. 

![](https://cloud.overment.com/2024-10-04/aidevs3_voice-11c7027d-a.png)

## Strukturyzowanie i formatowanie tekstu z audio

Widzimy już dość dokładnie, jak multimodalność radzi sobie w praktyce, zarówno w tym pozytywnym jak i negatywnym sensie. Ostatecznie pomimo tych wszystkich ograniczeń, nadal śmiało możemy mówić o budowaniu rozwiązań wnoszących wartość. Aczkolwiek obecnie często w poszukiwaniu jakości, musimy poświęcić czas reakcji. Nie zawsze jednak czas musi być istotny i może wynikać to bezpośrednio z naszych założeń projektowych.

Pierwszym z brzegu przykładem, mogą być **notatki głosowe** z których już teraz korzysta część osób, aczkolwiek wymaga to ich ręcznego przepisania lub ewentualnie pracy z transkrypcją, która nie zawsze jest doskonała. 

Problem ten zauważył twórca narzędzia [Audiopen](https://audiopen.ai/), które skupia się wyłącznie na formatowaniu podyktowanej notatki, dając przy tym możliwość dopasowania długości czy stylu. Choć nie można zaprzeczyć skuteczności wdrożenia tego pomysłu, tak na tym etapie jasne jest dla nas to, że główna logika działająca pod spodem, jest dość prosta do odwzorowania. 

![](https://cloud.overment.com/2024-10-04/aidevs3_audiopen-94e20f7f-1.png)

Co więcej, dzięki własnej implementacji, możemy pójść krok dalej i w pełni dopasować sposób transformacji nagrania audio. Jak się później przekonamy, będzie to także stanowić potencjalny element współpracującego z nami Agenta AI.

![](https://cloud.overment.com/2024-10-04/aidevs3_notes-7afdb309-3.png)

Największa wartość z wdrożenia powyższego schematu pochodzi z możliwości dopasowania go do swoich preferencji oraz nadanie mu naszego własnego kontekstu. W końcu każdy z nas może w inny sposób podchodzić do planowania zadań, zakupów czy prowadzenia notatek na spotkaniu. Poza tym z takim schematem można wyjść nieco dalej, np. w stronę prowadzenia dziennika czy planu treningowego. 

Poniższy fragment notatki został zbudowany na podstawie krótkiej wiadomości opisującej spotkanie z Grzegorzem na temat projektu Alice. 

![](https://cloud.overment.com/2024-10-04/aidevs3_note-6dfe1586-7.png)

Rzecz w tym, że w treści tej notatki nazwa ta nie padła ani razu, ale można było ją wywnioskować z kontekstu. Jej treść można zobaczyć w przykładzie `notes` wśród jednej z przykładowych wiadomości. 

![](https://cloud.overment.com/2024-10-04/aidevs3_meeting-cdd0b325-4.png)

Możliwość wnioskowania na podstawie kontekstu jest możliwa, ponieważ kontekst ten został wczytany po określeniu typu notatki (zadania/notatka ze spotkania/lista zakupów). Podobne scenariusze realizowaliśmy w przykładzie `linear` z lekcji S01E02 — Kontekst, gdzie zwracaliśmy uwagę na to, aby opisy poszczególnych kategorii były jasne lub abyśmy mogli poprowadzić model przez wykrycie dwuznaczności.

![](https://cloud.overment.com/2024-10-04/aidevs3_note_context-2ac1291d-b.png)

Dodatkowo, fakt, że zastosowaliśmy etap rozpoznania typu wiadomości, pozwala nam na wczytanie pasującego kontekstu oraz preferencji związanych z formatowaniem. Przekłada się to także na jeszcze jedną wartość w postaci **jednego punktu wejścia**. Oznacza to, że każda notatka głosowa lub tekstowa przekierowana do tego narzędzia umożliwi jej właściwe formatowanie i ewentualne umieszczenie w odpowiedniej kategorii w notatniku.
## Łączenie źródeł informacji z różnych formatów

Modele Vision coraz lepiej radzą sobie z rozpoznawaniem detali oraz wnioskowaniem na podstawie obrazu. W związku z tym, możemy je wykorzystać nawet w kontekście notowania jako źródło danych, lub nadanie kontekstu. Sam kontekst może być także nadawany poprzez dane pochodzące z urządzenia z pomocą którego tworzymy notatkę (np. lokalizacja telefonu). Zatem zaktualizowany schemat interakcji, wygląda następująco: 

![](https://cloud.overment.com/2024-10-04/aidevs3_enriched_note-f8f461f2-6.png)

Uwzględniamy w nim połączenie nagrania audio (lub wiadomości tekstowej), zdjęcia oraz metadanych z urządzenia. To wszystko może zostać przekazane do modelu, a następnie skojarzone z właściwą kategorią czy pamięcią długoterminową modelu. 

Warto przypomnieć zdolność VLM do dopasowywania opisów do obrazu, co widać poniżej, w wiadomości która poprawnie rozpoznaje moje biuro. Dokładnie w ten sam sposób możemy kojarzyć miejsca czy przedmioty (np. paragony).

![](https://cloud.overment.com/2024-10-04/aidevs3_recognize-7348bba2-1.png)

Faktyczną implementacją łączenia obrazu i metadanych urządzenia nie będziemy się już zajmować, ponieważ będzie ona zależała od systemu operacyjnego i samego urządzenia. Przykładowo w przypadku iPhone do dyspozycji mamy aplikację Shortcuts, która umożliwia stworzenie makra pozwalającego na zrobienie zdjęcia, nagrania wiadomości głosowej i wysłania tych danych na wskazany adres. Jak widać poniżej, jest to niesamowicie proste.

![](https://cloud.overment.com/2024-10-04/aidevs3_shortuct-a52076df-2.png)

## Podsumowanie

Przykłady omówione w tej lekcji miały na celu przede wszystkim pokazanie schematu myślenia, który dąży do kreatywnego, a jednocześnie użytecznego łączenia umiejętności modeli językowych oraz funkcjonalności otaczających nas urządzeń.

Nawet jeśli żaden z nich nie wpisuje się w Twoją codzienność lub Twój kontekst zawodowy, to zastanów się jakie narzędzie możesz opracować na własne potrzeby. Jest to o tyle ważne, że pozwoli na praktyczne doświadczanie możliwości obecnych modeli. Dodatkowo tak proste rozwiązania zwykle umożliwiają także dość proste przełączenie się na nowe wersje modeli. 

Największą wartością z powyższych możliwości, a jednocześnie pewną generalizacją tego, o czym mówimy, jest **transformacja treści** pomiędzy różnymi formatami. Nierzadko w praktyce spotykamy sytuacje w których jakieś dokumenty (w tym wiadomości e-mail) muszą być uspójnione (np. do formatu CSV) czy kompleksowo opisane. Wówczas rola dużych modeli językowych okazuje się znacząca, nawet pomimo potencjalnych błędów, które popełniają. Czas potrzebny na weryfikację jakiejś treści, jest zawsze zwykle mniejszy, niż jej stworzenie od podstaw.

Jeśli z tej lekcji masz wynieść tylko jedną rzecz, to po prostu rozejrzyj się wokół i wskaż jeden przykład, jak multimodalne modele mogą Ci pomóc w codziennym życiu. Podziel się swoim pomysłem w komentarzu.
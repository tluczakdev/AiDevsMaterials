![](https://cloud.overment.com/S02E04-1731493359.png)

Na pewnym etapie multimodalność najprawdopodobniej umożliwi nam generowanie tekstu, audio, obrazu, a być może nawet wideo. Przykładem może być [Advanced Voice Mode](https://www.youtube.com/watch?v=MirzFk_DSiI). Mówimy więc tutaj o możliwości interakcji **speech-to-speech**, a także innych konfiguracjach. 

Z drugiej strony, może nam zależeć także na interakcji z modelami wyspecjalizowanymi w określonych zadaniach lub po prostu nie posiadającymi możliwości przetwarzania wielu formatów. 

W obu przypadkach dążymy do transformacji danych występujących w różnych formatach. Niekiedy będą to obszerne bazy produktów, które wymagają opisu. Innym razem będzie to luźna notatka głosowa, potrzebująca ustrukturyzowania czy film z którego będziemy chcieli pobrać jakieś informacje. 
## Generowanie tekstu i audio

W lekcji S01E03 — Limity wspominałem, że duże modele językowe znacznie lepiej radzą sobie z transformacją treści, niż jej generowaniem. Mowa tutaj o sytuacji w której **na podstawie kilku słów opisu oczekujemy rozbudowanego dokumentu** zachowującego spójność stylu oraz wysoki poziom merytoryczny. Nie oznacza to jednak, że LLM nie są zdolne do tworzenia rezultatów wysokiej jakości, aczkolwiek wymaga to nieco większego zaangażowania w stworzenie logiki prowadzącej model. 

Dobrym przykładem takiego prowadzenia modelu jest **podsumowanie treści**, które jest też często prezentowane jako jedna z podstawowych umiejętności modelu. Na poniższej animacji widzimy dwa podsumowania: pierwsze wygenerowane przez prosty prompt z prośbą o streszczenie jednej z lekcji AI_devs. Natomiast po prawej mamy wynik działania przykładu `summary`, który buduje podsumowanie wieloetapowo, uwzględniając także linki oraz obrazki przeplatające treść we właściwych miejscach (co nie zawsze jest oczywiste dla modelu). 

![](https://cloud.overment.com/aidevs3_summary-1727679904.gif)

Mówimy tutaj o kompresji na poziomie ~55%. Co więcej, podsumowanie podzielone jest na części (wprowadzenie, omówione tematy, kluczowe wnioski), więc można zapoznać się tylko z interesującymi nas fragmentami. 

Pytanie zatem: **jak stworzyłem taki proces?**

Przede wszystkim, wyszedłem od pytania **jak sam napisałbym takie podsumowanie**, lecz zamiast odpowiadać na nie samodzielnie, omówiłem je z Claude 3.5. Nie wydarzyło się to jednak w ramach jednego wątku, lecz kilku:

- Na początku skorzystaliśmy z modelu mentalnego "First-principles Thinking" (myślenie zasadami pierwszego rzędu) w celu zidentyfikowania kroków potrzebnych do stworzenia dobrego podsumowania. Rezultat zawierał przydatne wskazówki (np. scoring czy modelowanie tematów) na które sam bym nie wpadł. Jednak ogólny proces mi się nie podobał, więc **poprosiłem o wygenerowanie notatki z naszej rozmowy, którą wykorzystałem w celu rozpoczęcia nowego wątku**.

![](https://cloud.overment.com/2024-09-30/aidevs3_summary-process-3d8cc824-f.png)

- Nowa konwersacja rozpoczęła się od wygenerowanej notatki z poprzedniej rozmowy. Pozwolił on na przekazanie kontekstu, co stanowiło podstawę do omówienia każdego z kroków indywidualnie. **Sugestie ze strony modelu były różne i większość z nich nie miała większego sensu**, ale ponownie pojawiły się wskazówki na które sam bym nie wpadł. Sama rozmowa pozwoliła mi także ustrukturyzować własne myśli. 

![](https://cloud.overment.com/2024-09-30/aidevs3_summary-process-3-8930bac0-c.png)

W rezultacie doszedłem do poniższego opisu głównych kroków (extract, draft, refine, critique, summarize, refine) oraz w przypadku tego pierwszego, także podpunktów. Opracowany plan wyglądał w porządku, więc wystarczyło przełożyć go na kod. 

![](https://cloud.overment.com/2024-09-30/aidevs3_structure-a14f6dff-f.png)

W pliku `app.ts` przykładu `summarize` znajduje się funkcja, która równolegle dokonuje niezależnej ekstrakcji informacji z każdej kategorii. W ten sposób budujemy **kontekst na podstawie którego generowana jest pierwsza wersja podsumowania**. Następnie jest ono **krytykowane** z uwzględnieniem zarówno oryginalnej treści artykułu oraz wszystkich pobranych informacji. 

Szkic, zebrany kontekst oraz krytyka zostają następnie przekazane do promptu piszącego główną wersję podsumowania, która finalnie przechodzi jeszcze rewizję pod kątem stylu. 

![](https://cloud.overment.com/2024-09-30/aidevs3_summarize-7d00de5a-a.png)

Zbierając to wszystko w całość: 

- Modele językowe obecnie są dość ograniczone pod kątem wykonywania złożonych zadań w ramach jednego zapytania. Z drugiej strony, jeśli skupimy ich uwagę tylko na jednej aktywności, to skuteczność jest bardzo wysoka i w zależności od zadania, może osiągać 100%
- Przetwarzany przez model tekst, może być analizowany wielokrotnie pod różnym kątem. Pozwala to na automatyczne zgromadzenie wysokiej jakości kontekstu, na podstawie którego może być wykonane docelowe zadanie
- Warto jednak rozważyć dodatkowe kroki, w których wygenerowana treść zostanie poddana krytyce, czy nawet weryfikacji pod kątem naszych wymagań. Tak wygenerowany feedback pozwoli wyeliminować dużą część błędów obecnych w pierwszej iteracji
- Finalne podsumowanie powinno już posiadać docelową strukturę oraz komplet informacji. Jednak ze względu na fakt, że LLM domyślnie stosują raczej mało precyzyjny styl wypowiedzi, warto wykonać jeszcze jeden krok, który skupi się już tylko na stylizacji treści
- **WAŻNE:** Wymienione powyżej kroki nie muszą być wykonywane przez jeden model. Wprost przeciwnie, wskazane jest łączenie ze sobą różnych modeli — od tych mniejszych i tańszych, zdolnych do wykonania prostych operacji, po te najbardziej zaawansowane, zdolne do budowania złożonych wypowiedzi. 

Efekty można zobaczyć po uruchomieniu promptu lub poprzez wgląd w treść plików markdown, obecnych w katalogu `summary`. Tymczasem chciałbym zwrócić uwagę na jeszcze jeden element zastosowany w kodzie, a mianowicie **stwarzanie modelowi czasu na myślenie** poprzez zastosowanie tagów `xml-like`, wewnątrz których zostaje dodana odpowiedź po wcześniejszym zastanowieniu. Jest to praktyczne połączenie informacji z [Let me speak Freely?](https://arxiv.org/abs/2408.02442v1), technik sugerowanych przez Anthropic oraz Chain of Thought.

![](https://cloud.overment.com/2024-09-30/aidevs3_speakfreely-e483c400-4.png)

Ostatecznie, prompty wykorzystane przy generowaniu podsumowań, mogą być dopasowane do nas. Można też pomyśleć o przygotowaniu instrukcji na tyle uniwersalnych, aby mogły sprawdzać się w różnej kolejności. 

Trzeba także zaznaczyć, że przykład `summarize` jest zdolny do generowania podsumowań o ograniczonej długości i wynika to zarówno z limitów `input` jak i `output` tokens. Dodatkowo możliwe są także problemy związane z przepisywaniem linków i pojawiającymi się w nich literówkami. Natomiast do tego tematu wrócimy w dalszej części AI_devs. 
## Opisywanie obrazów i wideo

W podsumowaniu generowanym przez przykład `summary` uwzględnione są obrazki, a konkretnie linki, które do nich kierują. Zgodnie z lekcją S01E02 — Kontekst, przechowywanie plików na własnym serwerze jest kluczowe, aby model mógł się nimi posługiwać w swoich wypowiedziach, korzystając z formatu markdown. 

Same linki nie są wystarczające, ponieważ w takiej formie, model nie może podejrzeć tego, co znajduje się na obrazie. Musimy więc pobrać treść pliku i dostarczyć do modelu w formie base64.

Podobnie jak w przypadku podsumowania, potrzebujemy dodatkowego kontekstu, aby wygenerować wartościowy opis. Informacje których potrzebujemy obejmują **podgląd obrazu** oraz **kontekst w postaci treści, która go otacza**, które model może połączyć w całość. 

![](https://cloud.overment.com/2024-09-30/aidevs3_captions-22c82adb-0.png)

Mówiąc konkretnie: 

1. Wczytujemy treść dokumentu
2. Z pomocą wyrażeń regularnych pobieramy wszystkie obrazy i wczytujemy je w formie base64
3. Gromadzimy kontekst poprzez dwa zapytania do modelu: **podgląd** i **kontekst**
4. No i na końcu łączymy obie te informacje

Poniżej znajduje się przykład w którym widać wyraźnie połączenie wizualnej interpretacji obrazu, wraz z otaczającym go kontekstem. Jest to wynik działania przykładu [captions](https://github.com/i-am-alice/3rd-devs/tree/main/captions) 

![](https://cloud.overment.com/2024-09-30/aidevs3_caption-6a9ce063-8.png)

Tak wygenerowane opisy mogą być wykorzystane w przykładzie `summary` w celu wzbogacenia kontekstu na podstawie którego generujemy opis całego artykułu. 

Co więcej, nie jesteśmy tutaj ograniczeni wyłącznie do obrazów, ponieważ w przypadku modelu Gemini 1.5 Pro, możemy przeanalizować także materiał wideo. Poniżej mamy przykład z [Google AI Studio](https://aistudio.google.com/) w którym na podstawie fragmentu nagrania o ComfyUI, model poprawnie wymienił wspominane narzędzia. 

![](https://cloud.overment.com/2024-09-30/aidevs3_video-5c2fd96e-2.png)

Schemat opisywania materiałów wideo wygląda podobnie jak w przypadku obrazów i Google AI Studio udostępnia API, które pozwoli nam ten proces zautomatyzować. Trzeba tylko zwrócić uwagę na liczbę tokenów, która dla jednominutowego materiału wideo wyniosła ponad 18 000. Sama interakcja z API Google AI Studio znajduje się w przykładzie `video`, który zawiera dwa pliki testowe `test.mp3` z krótkim nagraniem oraz `test.mp4` z kilkusekundowym filmem. Aby przełączyć się pomiędzy nimi, wystarczy zmiana nazwy oraz `mimeType` w funkcji `processMedia`. 

![](https://cloud.overment.com/2024-10-01/aidevs3_vid-14869acb-8.png)

**WAŻNE:** Google AI Studio nie oferuje możliwości łatwej kontroli kosztów, a jedynie limity liczby zapytań dla poszczególnych usług. W przykładzie `video` uwzględniłem limit wielkości pliku wynoszący 2MB, jednak samo API pozwala na przesyłanie plików o wadze nawet 2GB.

Ostatnią ważną informacją w kontekście pracy z modelami Gemini, jest możliwość wykorzystania pamięci podręcznej dla kontekstu, co w przypadku wielokrotnego przetwarzania plików audio/wideo jest koniecznością. 
## (częściowo)Autonomiczna Współpraca różnych modeli

W dotychczasowych przykładach wykorzystywaliśmy różne modele, ale współpraca między nimi miała charakter indywidualny. Na przykład jeden model generował tekst, a drugi zamieniał go na audio, lub odwrotnie. Choć w takiej sytuacji nie ma nic złego, możemy wyjść znacznie ponad to.

Mianowicie, umiejętności modeli możemy połączyć programistycznie w taki sposób, aby wzajemnie ze sobą współpracowały, dążąc do możliwie najlepszego rezultatu. Namiastkę tego widzieliśmy w przykładzie `summary`, gdzie model `o1-mini` weryfikował pracę `gpt-4o`, ale tam poruszaliśmy się tylko w obszarze tekstu, wchodząc nieznacznie w analizę obrazu.

**UWAGA:** Przykład narration opiera się o obecnie eksperymentalne możliwości ElevenLabs dotyczące generowania efektów dźwiękowych. Korzystanie z nich jest obecnie dość drogie. W katalogu tego przykładu znajdują się przykładowe pliki prezentujące efekt jego działania. 

Natomiast w przykładzie `narration` modele `o1-mini` (OpenAI), `eleven_multilingual_v2` (ElevenLabs) oraz `gemini-1.5-flash` (Google DeepMind) otrzymują zadanie w postaci wygenerowania narracji na dany temat, wzbogaconej o efekty dźwiękowe.

![](https://cloud.overment.com/2024-10-01/aidevs3_narration-1f10481c-f.png)

Taki scenariusz prawdopodobnie już niebawem będzie bardzo prosty do zrealizowania z pomocą modeli multimodalnych, które być może będą zdolne do generowania dźwięku. Natomiast na dzień pisania tych słów, jest on niewykonalny z pomocą jednego modelu. 

Mówimy tutaj o następującym scenariuszu:

1. Użytkownik przesyła do modelu temat lub narrację, a model ma wygenerować odpowiedź w postaci narracji wzbogaconej o efekty dźwiękowe
2. W pierwszym kroku z pomocą modelu wzbogacamy tekst o `[opisy dźwięków]`, które mogą oddawać aktualną scenę, np. "Szedł nocą po śniegu `[spokojny chód po zaśnieżonym chodniku]`, gdy nagle (...)"
3. Następnie programistycznie segmentujemy tekst, grupując fragmenty wypowiedzi lektora oraz efekty dźwiękowe
4. Mając tak ułożone dane, możemy skorzystać z Text to Speech oraz Text to Audio, aby wygenerować ścieżkę dźwiękową, która może być złożona w całość

Nie brzmi to zbyt skomplikowanie, gdyby nie fakt, że jakość modeli Text to Audio jest obecnie niska i trudno za pierwszym razem wygenerować nagranie, które spełnia nasze oczekiwania. Tutaj do gry wchodzi `gemini-1.5-flash`, mający zdolność do rozpoznawania nie tylko mowy (tak jak modele Speech to Text), ale także dźwięku. Poniżej widzimy jak we właściwości `_thinking` zostaje określony ogólny nastrój nagrania i fakt, że nie pasuje ono w pełni do oczekiwanego stylu. Taka obserwacja prowadzi do podjęcia decyzji o tym, aby ponownie wygenerować ten fragment. 

![](https://cloud.overment.com/2024-10-02/aidevs3_listen-ab3d4eb1-0.png)

Przykłady wygenerowanych w ten sposób narracji znajdują się w katalogu `narration`, a jeden z nich można odsłuchać poniżej. W przypadku chęci przetestowania tego mechanizmu, konieczne będzie uzupełnienie kluczy w Google AI Studio oraz ElevenLabs, a następnie podmiana wiadomości użytkownika z funkcji `generateNarration` pliku `narration/app.ts`.

<div style="padding:100% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1015122040?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="02_04_narration"></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

Produkcyjne zastosowanie tego mechanizmu, który miałby działać na nieco większej skali, **wymagałoby interfejsu użytkownika pozwalającego na ręczne wprowadzenie poprawek**. Pełna automatyzacja byłaby możliwa, pod warunkiem dostępu do bardziej precyzyjnego modelu odpowiedzialnego za efekty dźwiękowe. 

Ostatecznie przykład ten, ma na celu **podkreślenie unikatowej wartości, która płynie z połączenia ze sobą wielu modeli.** No bo analogicznie możemy spojrzeć na pozostałe formaty, budując narzędzia zdolne do generowania raportów, podcastów (podobnie jak [NotebookLM](https://notebooklm.google/)) czy wideo.

Kilka przykładów: 

- Multimodalny RAG pozwalający na pracę z różnymi formatami, w tym także wideo, a nawet nagraniami w czasie rzeczywistym
- Wieloformatowy asystent nauki, zdolny zarówno do analizowania, jak i generowania różnych formatów. Dzięki narzędziom takim jak [markmap](https://markmap.js.org/) czy [mermaid](https://mermaid.js.org/) mógłby wizualizować koncepcje z pomocą grafów i map myśli
- Zadania takie jak rozpoznawanie mowy czy obrazów mogą być realizowane równolegle przez różne modele, które wykazują różną skuteczność w zależności od sytuacji. Pozwala to zwiększyć skuteczność tam, gdzie jeden model nie jest wystarczający
- Sam proces rozumowania realizowany przez kilka modeli jednocześnie, również przekłada się na znaczną poprawę skuteczności. Przykładem jest [Mixture of Agents](https://arxiv.org/abs/2406.04692) w przypadku którego modele Open Source są zdolne do uzyskania lepszych rezultatów niż np. GPT-4o.

![Struktura Mixture of Agents](https://cloud.overment.com/2024-10-02/aidevs3_mixture_of_agents-5f12200a-f.png)

## Podsumowanie

Multimodalność znacznie zwiększa użyteczność modeli generatywnego AI, które wychodząc poza sam tekst są w stanie samodzielnie realizować zadania, które wcześniej wymagały zaangażowania człowieka. Z drugiej strony, obecny rozwój modeli nie pozwala jeszcze na swobodne adresowanie dowolnego problemu. W dodatku modele od czasu do czasu generują treści zawierające błędy, co sprawia, że zaangażowanie człowieka na którymś etapie procesu jest niezbędne. 

Prawdopodobnie największym wyzwaniem związanym z praktycznym zastosowaniem multimodalności i tak pozostaje nasza **kreatywność** oraz programistyczne zdolności. Jest to związane z faktem, że mówimy tutaj o zupełnie nowych możliwościach, w przypadku których dość trudno jest "połączyć kropki". Dochodzi do tego aspekt samej technologii i zastosowania API oraz narzędzi z którym na co dzień nie mamy potrzeby pracować (np. WebSockety).

Najważniejszym tematem tej lekcji jest zaawansowane przetwarzanie dokumentów przedstawione w przykładzie `summary`. Zatem jeśli chcesz zabrać ze sobą tylko jedną rzecz dziś, to uruchom go i przetestuj działanie na jakimś wpisie z bloga, który czytasz i sprawdź jakość podsumowania. Jeśli rezultat nie będzie zgodny z Twoimi oczekiwaniami, zastanów się, co możesz zmienić, aby do niego doprowadzić.

Powodzenia!
![](https://cloud.overment.com/S05E01-1733049915.png)

Nawiązania do agentów AI pojawiały się już wielokrotnie w dotychczasowych lekcjach, ale dopiero teraz jesteśmy gotowi, aby w pełni zająć się tym tematem. 

Zacznijmy od tego, że definicja "agenta AI" jest dość płynna. W lekcji S00E04 — Programowanie wspominałem, nazywamy tak systemy zdolne do autonomicznego wykonywania zadań. W kontekście generatywnego AI, mówimy o aplikacji w której LLM połączony jest z narzędziami oraz pamięcią długoterminową (określa się to mianem [Augmented Language Models](https://arxiv.org/pdf/2302.07842), ALM). 

Na podstawie tej definicji można uznać, że praktycznie wszystkie omawiane przez nas przykłady to rodzaj agenta. Nie będzie to błędne stwierdzenie, ponieważ rzeczywiście mówimy tam o "pewnym stopniu agencyjności". Zatem określenie "agent" nie jest rzeczownikiem, lecz czasownikiem określającym poziom autonomiczności systemu. Dla jasności — to stwierdzenie pochodzi od [Andrew Ng](https://x.com/AndrewYNg) i jest raczej opinią, niż definicją.

Ta lekcja rozpoczyna ostatni rozdział AI_devs, którego celem jest połączenie wszystkiego, czego nauczyliśmy się o pracy z modelami i budowaniu narzędzi. Nadszedł czas, by stworzyć system zdolny do autonomicznego działania, wymagający minimalnego zaangażowania człowieka.
## Główne założenia Agenta AI

W przeciwieństwie do narzędzi, które budowaliśmy do tej pory, Agent może samodzielnie decydować o tym, w jaki sposób podejść do zadania oraz **kiedy je zakończyć**. Oznacza to, że w chwili otrzymania polecenia, uruchomiona zostaje **pętla** o wyjściu z której decyduje model (lub limit narzucony programistycznie).

W przykładzie `agent` znajduje się logika zawierająca taką pętlę. Widzimy w niej, że agent posiada swój stan, który aktualizuje się przy każdej kolejnej iteracji. Natomiast iteracja zaczyna się od **zaplanowania kolejnego ruchu** oraz **uruchomienia narzędzia**. Gdy wybranym narzędziem zostaje "final_answer", to pętla zostaje przerwana i agent przesyła wiadomość do użytkownika. 

> Ważne: Na potrzeby przykładu `agent`, stan resetuje się dopiero po ponownym uruchomieniu serwera Node.js. 

![](https://cloud.overment.com/2024-10-25/aidevs3_agent-7598170c-c.png)

Po wysłaniu prostej wiadomości "Hey, what's up?", agent nie odpowiada od razu, ale rozważa dobór odpowiedniego narzędzia. Ponieważ pytanie nie wymaga dodatkowych działań, zostaje wybrane narzędzie **final_answer**. Decyzja ta przerywa pętle (poleceniem "break" w 52 linii) i przechodzi do wygenerowania odpowiedzi.

![](https://cloud.overment.com/2024-10-25/aidevs3_simple-55e35167-e.png)

W sytuacji, gdy zapytanie użytkownika będzie wykraczało poza umiejętności agenta, to również zostanie wybrana akcja "final_answer" z informacją, że polecenie nie może być zrealizowane ze względu na ograniczone możliwości. Warto mieć to na uwadze i projektować system tak, aby był w stanie odnaleźć się w sytuacji wykraczającej poza zakres z myślą o którym został stworzony.

![](https://cloud.overment.com/2024-10-25/aidevs3_music-9085491e-3.png)

Przykład `agent` uwzględnia możliwość skorzystania z drugiego narzędzia, pozwalającego na przeszukiwanie wybranych stron www. Jest to dokładnie ta logika, którą omawialiśmy w przykładzie `websearch` z lekko zmodyfikowanymi promptami. Poniżej widzimy, że prośba o wskazanie najnowszych wydarzeń z Anthropic i OpenAI została wykonana poprawnie i zawiera aktualne (na dzień pisania tych słów) informacje. 

![](https://cloud.overment.com/2024-10-25/aidevs3_search-3476e86a-e.png)

Ważny jest wspomniany "stan", który umożliwia zadawanie pogłębiających pytań. W przedstawionym przykładzie poprosiłem o pięć wpisów z każdego źródła. Agent, mając już wczytaną treść tych stron, wykorzystał posiadane informacje zamiast ponownego przeszukiwania sieci.

![](https://cloud.overment.com/2024-10-25/aidevs3_followup-2a5b8c8c-2.png)

Ostatecznie przykład `agent` jest bardzo prostą implementacją, która przedstawia tylko najważniejsze mechaniki. Konkretnie są to:

- **Planowanie:** Agent dąży do zrozumienia swojej bieżącej sytuacji w kontekście dostępnej wiedzy oraz narzędzi. Następnie podejmuje decyzję o następnym kroku
- **Pamięć:** Agent posiada pamięć obejmującą zarówno wymienione wiadomości, ale także podjęte akcje
- **Narzędzia:** Agent ma do dyspozycji narzędzia, po które może sięgnąć w razie potrzeby

Połączenie "rozumowania" oraz "akcji" o którym tutaj mówimy, nawiązuje do wzorca ReAct obejmującego **rozumowanie, obserwację i działanie**. Aczkolwiek w naszym przypadku planowanie i obserwacja odbywa się w ramach jednego kroku.

Prowadzi nas to wszystko do pytania: **W jaki sposób powinien zbudowany być agent?** 

Nie ma tutaj jednoznacznej odpowiedzi, ale mamy do dyspozycji wzorce oraz różne techniki pracy, które możemy wykorzystać w różnych konfiguracjach. 

Ostatecznie nadrzędnym celem zwykle będzie to, aby agent był w stanie **rozbić zadanie na mniejsze akcje, zrealizował je i połączył uzyskany efekt w finalną odpowiedź**.
## Wzorce Agencyjne

Na blogu [deeplearning.ai](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/) znajduje się seria wpisów na temat wzorców agencyjnych oraz ich elementów takich jak: **Refleksja, Narzędzia, Planowanie** oraz **Zespoły Agentów**. Jeśli masz za sobą wcześniejsze edycje AI_devs to pierwsze trzy z nich są Ci już znane, ale i tak teraz przyjrzymy się im z perspektywy logiki agenta. No bo teraz znaczącą różnicą jest fakt, że działają one w pętli, co otwiera przestrzeń do autonomicznego rozwiązywania złożonych problemów. Przyjrzyjmy się więc im bliżej:

- **Refleksja:** to proces zastanawiania się nad aktualną sytuacją.
- **Narzędzia:** obejmuje decyzję o tym, które narzędzie wybrać oraz w jaki sposób, a także jak wykorzystać otrzymany rezultat
- **Planowanie:** polega na ustaleniu listy akcji, lub decyzji o kolejnym kroku, w oparciu o posiadane informacje i postawiony cel
- **Zespół Agentów:** polega na nawiązaniu współpracy pomiędzy wyspecjalizowanymi agentami, którzy komunikując się ze sobą, dążą do realizacji wspólnego celu.

Zanim przejdziemy dalej, zaznaczę tylko, że w tej lekcji skupimy się na najważniejszych wątkach projektowania agentów. Natomiast sam ten temat jest bardzo rozległy i obecnie znajduje się jeszcze w obszarze eksploracji. Praktycznie każdego miesiąca pojawiają się nowe publikacje, które wnoszą coś nowego lub kwestionują dotychczasowe techniki pracy. Część z nich znajdziemy w repozytorium [Awesome LLM-Powered Agent](https://github.com/hyp1231/awesome-llm-powered-agent?tab=readme-ov-file#general-reasoning--planning--tool-using). 

![](https://cloud.overment.com/2024-10-25/aidevs3_papers-1476a26e-1.png)

## Refleksja

Zacznijmy od **refleksji**, która w przypadku ludzi zwykle odbywa się w ciszy i niekiedy wspierana jest przez narzędzia (np. kartkę papieru czy edytor tekstu). Takie zastanawianie się pozwala nam lepiej zrozumieć dany temat i **zwiększa szansę na to, że podejmiemy właściwą decyzję**.

Dokładnie taki sam cel ma refleksja w przypadku agentów AI. Wiemy jednak, że modele językowe w obecnej formie mają bardzo ograniczony czas "na myślenie" przy generowaniu pojedynczego tokenu. Inaczej mówiąc, model "myśli" **generując kolejne tokeny**. Dokładnie z tego powodu stosowaliśmy właściwości `_thinking` przy generowaniu obiektów JSON lub tag `<final_answer>` w przypadku wypowiedzi niewymagających ustrukturyzowanej odpowiedzi. Natomiast refleksja może przyjmować różne formy i być znacznie bardziej rozbudowana niż to, przez co przechodziliśmy do tej pory i **może przyjmować nawet formę oddzielnego agenta** czego przykład został opisany w publikacji "[Improving LLM Reasoning with Multi-Agent Tree-of-Thought Validator Agent](https://arxiv.org/pdf/2409.11527)".

![](https://cloud.overment.com/2024-10-25/aidevs3_reasoner-c79ff27f-f.png)

Refleksja nie ma jednak na celu wyłącznie generowania dodatkowych tokenów, lecz tokenów, które będą **sprzyjały uzyskaniu poprawnej odpowiedzi**. Będziemy chcieli więc zmniejszyć ryzyko pojawienia się tam informacji, które będą działać na naszą niekorzyść.

Domyślnie "myślenie" w wykonaniu modelu jest dość ogólne, rozbudowane i zawiera wiele zbędnych treści. Poniższy przykład dobrze to obrazuje, ponieważ refleksja jest niemal dokładnym powtórzeniem odpowiedzi (czyli sekcji oddzielonej nową linią), aczkolwiek widoczny tutaj fragment został lekko przycięty. 

![](https://cloud.overment.com/2024-10-25/aidevs3_thinking-3367785e-2.png)

> Ciekawostka: Chain of Thought może także **negatywnie** wpływać na skuteczność działania modelu dla niektórych zadań, co zostało opisane w publikacji "[Mind Your Step (by Step)](https://arxiv.org/abs/2410.21333)". Mowa tam konkretnie o "zero-shot chain of thought", czyli sytuacji w której model "myśli krok po kroku" bez większych sugestii z naszej strony. Okazuje się, że takie podejście nie sprawdza się w przypadku wybranych klasyfikacji, szczególnie tych, które uwzględniają jakieś odchylenia od normy.

Dla porównania, poniżej znajduje się fragment mojej rozmowy w Cursor dotyczący wprowadzenia poprawek w kodzie. Model nie przeszedł bezpośrednio do pisania kodu. W pierwszej kolejności wymienił elementy, z którymi będzie pracować, a następnie **wskazał słowa kluczowe koncepcji**, które mogą być istotne przy wykonaniu tego zadania — Single Responsibility Principle czy Dependency Injection.

![](https://cloud.overment.com/2024-10-25/aidevs3_refactor-37a23854-d.png)

W pliku `agent/WebService.ts` znajduje się metoda `generateQueries`, której prompt także zawiera element refleksji, ale jej treść różni się od wcześniejszych. Jest zwięzła, precyzyjna i skupia się na faktach dostępnych w kontekście. 

![](https://cloud.overment.com/2024-10-25/aidevs3_queries-79d22bc4-1.png)

Powodem są przykłady zawarte w prompcie, które model naśladuje w swoich odpowiedziach. Ich treść wynika bezpośrednio z moich doświadczeń oraz z celu, który jest osadzony w konkretnym kontekście posługiwania się wyszukiwarką.

![](https://cloud.overment.com/2024-10-25/aidevs3_examples-0362ff29-4.png)

Także myśląc o "refleksji" modelu, musimy zawsze mieć na uwadze **wspieranie** procesu rozumowania poprzez dążenie do otrzymania treści, które **zwiększą szansę na uzyskanie oczekiwanego rezultatu**. Będzie więc nam zależało na tym, aby model **nawiązał do słów kluczowych, koncepcji lub wymienił wątki na których ma się skupić jego uwaga**.

Trzeba tylko pamiętać o tym, że **sterowanie refleksją** w sposób widoczny na ostatnim przykładzie, może mieć negatywny wpływ na działanie modelu, ponieważ nie będzie mieć on szansy na eksplorację ścieżek, o których nie pomyśleliśmy. Warto więc zachować ostrożność.

Ogólną zasadą, którą można kierować się przy kształtowaniu procesu refleksji, jest zastanowienie się nad tym, **jak sami byśmy podeszli do rozwiązania problemu** oraz **jak przekłada się to na możliwości modelu**. Wówczas zauważamy to, które rzeczy mają zostać wzięte pod uwagę oraz jakich danych potrzebuje model.
## Narzędzia

Mając przygotowane narzędzia posiadające wspólny interfejs o którym mówiliśmy w S04E01 — Interfejs, połączenie ich z agentem, staje się stosunkowo proste. To właśnie dlatego niemal zawsze dbaliśmy o to, by dane wejściowe każdego z nich miały formę **zapytania tekstowego**, a dane wyjściowe — formę dokumentów.

Jak dotąd mieliśmy już przykłady, np. `web` lub `todo` w których LLM posługiwał się narzędziami. Natomiast w obu przypadkach, obecna tam mechanika była wyspecjalizowana w jednym obszarze. Natomiast w przypadku agentów, zakres ten będzie nieco większy. 

Dla przypomnienia, przykład `todo` składał się z trzech akcji — planowania, wykonania i odpowiedzi. Dodatkowo sam etap planowania generował nie tylko listę niezbędnych kroków, ale również parametry potrzebne do uruchomienia każdej z akcji. 

![](https://cloud.overment.com/2024-10-25/aidevs3_basic-2001f278-f.png)

Takie podejście nie wystarczy, gdy agent będzie dysponował większą liczbą narzędzi, ponieważ złożoność promptu nie pozwoli na skuteczne generowanie odpowiedzi. 

Natomiast w przykładzie `agent` logika ta uwzględnia dodatkowy krok związany z **przygotowaniem parametrów** do uruchomienia narzędzia, na podstawie instrukcji obsługi. Pozwala to na **skupienie uwagi modelu** na jednym zadaniu, co pozytywnie przekłada się na precyzję wypowiedzi.

![](https://cloud.overment.com/2024-10-25/aidevs3_logic-eca8808e-3.png)

Spójrzmy teraz na to, co się dzieje w chwili, gdy przykład `agent` zostanie poproszony o odnalezienie filmu na temat "Teorii Gier" z Yale University:

- Agent zastanawia się nad tym, że brak informacji na temat tego filmu w jego zasobach, wymaga skorzystania z narzędzia "web_search", które uruchamia z prostym zapytaniem do wyszukiwarki.
- Wewnątrz narzędzia "web_search" dochodzi do serii zapytań, które dążą do zawężenia zapytania do wskazanego źródła oraz wygenerowania podzapytań jeśli istnieje taka potrzeba
- Wyniki wyszukiwania zostają zamienione na dokumenty i dodane do pamięci krótkoterminowej agenta

![](https://cloud.overment.com/2024-10-25/aidevs3_theory-1d8f2190-1.png)

To wszystko wskazuje, że bez względu na liczbę narzędzi, dopóki ich interfejs pozostaje spójny, logika agenta będzie wystarczająca do efektywnego wykorzystania każdego z nich.

Co więcej, w przykładzie `agent`, w przeciwieństwie do wcześniejszych, istnieje możliwość **zareagowania na ewentualny problem** i jeśli pobrane informacje będą niewystarczające, to agent może podjąć decyzję o dalszym poszukiwaniu, lub poproszeniu o pomoc człowieka. Mamy więc tu pewien stopień autonomii oraz elastyczności, pozwalającej dopasować się do bieżącego problemu. 

Zatem temat posługiwania się narzędziami przez agenta obejmuje: 

- Etap planowania w którym model decyduje o tym, które narzędzie wybrać na podstawie jego nazwy oraz opisu, a także aktualnie posiadanych informacji.
- Etap **opisywania** parametrów potrzebnych do uruchomienia narzędzia na podstawie instrukcji.
- Etap **uruchomienia** narzędzia (co dzieje się już po stronie programistycznej)

Najważniejsze są tutaj dwie rzeczy: **precyzja opisów/instrukcji** oraz **poprawnie dostarczony kontekst**. Inaczej mówiąc, model powinien mieć możliwie jak najmniejszą przestrzeń do popełnienia błędu. Dobrze widać to w prompcie odpowiedzialnym za etap planowania, w którym znajduje się kontekst z dostępną wiedzą oraz historią podjętych kroków. Zwróć uwagę na:

- Składnię "xml-like", która wyraźnie oddziela sekcje kontekstu
- Atrybuty nadające kontekst dla każdej akcji. Np. description dla akcji websearch wygląda tak — `This is a result of a web search for the query: "${searchResult.query}"`. Zawiera więc informację o tym, dla jakiego zapytania są to wpisy.
- Wartości wskazujące na **brak rezultatów**, co może też sugerować, że akcja się nie powiodła i agent może nas o tym poinformować, bądź zdecydować o kolejnej próbie.

![](https://cloud.overment.com/2024-10-25/aidevs3_plan_context-6e56e72e-7.png)

W skrócie, w łączeniu agenta z narzędziami kluczowa jest **precyzja**. Choć może wydawać się to oczywiste, złożoność dynamicznego kontekstu rośnie bardzo szybko, co sprawia, że łatwo o błąd.

Akurat w naszym przypadku nie jest to teraz potrzebne, ale jeśli lista umiejętności agenta byłaby bardzo duża, można rozważyć indeksowanie ich w wyszukiwarce. Wówczas na liście dostępnych w danej chwili narzędzi pojawiałoby się tylko kilka wpisów, co również będzie mieć pozytywny wpływ na precyzję. Co więcej, przykłady użycia tych narzędzi mogłyby pojawiać się w prompcie **dynamicznie**, co jeszcze bardziej zwiększy skuteczność.
## Planowanie

W przykładzie `agent` etap planowania i refleksji są połączone. Jednak zwykle będziemy mieć potrzebę rozbicia tego procesu na dwie, oddzielne aktywności. Wówczas planowanie będzie skupiać się wyłącznie na tym, aby zestawić ze sobą **refleksję / przemyślenia na temat bieżącej sytuacji**, **posiadaną wiedzę** oraz **dostępne umiejętności** i na tej podstawie, podjąć dalszy plan.

W przypadku jednego z moich projektów, etap planowania (widoczny na screenie `next step`) poprzedzają aż **trzy rodzaje refleksji**. Pierwszy odpowiada za pobranie słów kluczowych na potrzeby wyszukiwania, drugi wskazuje obszary pamięci, które należy przeszukać, a trzeci skupia się na sugestii umiejętności, które warto wziąć pod uwagę. 

Wygenerowane wyniki zostają przekazane do etapu "planowania" jako **sugestie** na podstawie których zostaje podjęta ostateczna decyzja o następnej akcji. 

![](https://cloud.overment.com/2024-10-25/aidevs3_reasoning-cb419902-e.png)

Jasno widać tutaj jak poszczególne komponenty logiki agenta mogą być podzielone na wiele wyspecjalizowanych kroków, które mogą być uruchamiane tylko w wybranych sytuacjach.

Samo planowanie może także kojarzyć się z układaniem serii akcji, a nie wyborze tylko jednej z nich. Sam jednak nie korzystam z takiego podejścia ze względu na to, że: 

- **Zmienność:** W trakcie wykonywania akcji może dojść do nieprzewidzianych sytuacji, do których agent powinien się dostosować
- **Skupienie:** W danej chwili agent rozważa tylko kolejną akcję, a nie całą serię.
- **Historia:** Przy każdym kolejnym kroku agent posiada już informację o podjętych krokach, więc w razie potrzeby może się do nich odwołać

Etap planowania / podejmowania decyzji jest najważniejszym spośród wszystkich akcji podejmowanych przez agenta. Musimy więc zadbać o to, aby skuteczność działającego tutaj promptu, była możliwie jak najwyższa. Możemy to zrobić na trzy sposoby: 

- **Przykłady:** Few-Shot to najbardziej skuteczny sposób na zwiększenie skuteczności działania modelu i jest to pierwsza rzecz, którą powinniśmy rozważyć przy optymalizacji promptu.
- **Weryfikacja**: Dodatkowy prompt, który skupia się wyłącznie na ocenie podjętej decyzji, to wymagający, ale bardzo skuteczny sposób na wykrycie błędu
- **Fine-tuning:** Gdy mamy już działający system i zestaw danych, możemy wyspecjalizować model w podejmowaniu decyzji o kolejnym kroku

No i oczywiście w tym miejscu zawsze powinniśmy brać pod uwagę najlepszy dostępny model. 
## Rodzaje pamięci

Pamięć agenta zwykle podzielona jest na trzy kategorie:

- **Historia rozmów:** Obejmuje zarówno aktualną konwersację, jak i wcześniejsze interakcje
- **Dokumenty:** To zarówno dokumenty wgrane przez użytkownika, jak i rezultaty działania narzędzi
- **Pamięć długoterminowa:** To ogólna wiedza agenta na temat siebie, rozmówcy oraz otaczającego świata. Zwykle obejmuje informacje wykraczające poza bazową wiedzę modelu

Mówimy więc tutaj o bazie danych, której wpisy dodane są także do indeksów silników wyszukiwania. Moduł zwykle będzie pełnić rolę dodatkowego narzędzia, z którego agent będzie korzystać tak samo, jak w przypadku pozostałych umiejętności, a jego rezultatem będzie lista dokumentów wczytywana do kontekstu.

W przypadku pamięci, warto także rozważyć możliwość dostępu do niej poprzez API, co pozwoli na dostęp do pojedynczych wpisów oraz ich aktualizację, lub wyświetlenie w interfejsie użytkownika.

Pamięć jest źródłem wiedzy, dlatego zależy nam nie tylko na dostępie do niej, ale także na jej tworzeniu i aktualizowaniu. To spore wyzwanie, ponieważ:

- Musimy upewnić się, że nowe informacje zostaną dodane w sposób umożliwiający ich łatwe odnalezienie w przyszłości
- Podczas aktualizacji może dojść do utraty danych lub ich rozwarstwienia, na przykład poprzez utworzenie nowego wpisu zamiast aktualizacji istniejącego
- Przeszukiwanie istniejących wspomnień nie zawsze jest oczywiste, ponieważ zapytanie użytkownika może być niewystarczające, aby skutecznie połączyć je z wszystkimi istotnymi dokumentami.

Dlatego budując pamięć warto rozważyć: 

- **Struktura:** Budowanie wspomnień w oparciu o z góry zdefiniowaną strukturę, pozwala na późniejsze zawężenie wyszukiwania, co jak się przekonaliśmy już w lekcjach S03E02 — Wyszukiwanie semantyczne i S03E03 — Wyszukiwanie Hybrydowe, ułatwia dotarcie do pożądanych treści. Nadanie struktury jest też ograniczeniem, ale zwykle pamięć agenta i tak będzie skupiona wokół pojedynczych obszarów, a nie generalnej wiedzy na dowolny temat. 
- **Kontekst:** W przypadku pamięci, możemy stworzyć jedną notatkę, która będzie stanowić punkt odniesienia dla agenta przy posługiwaniu się pamięcią. Przykładowo: w kontekście może znajdować się lista naszych projektów, więc jeśli powiem o Tech•sistence, to agent od razu będzie wiedział, aby szukać informacji w obszarze "praca". Taki kontekst stanowi więc **esencję** wiedzy agenta.
- **Eksploracja:** Podobnie jak w przykładzie `websearch`, przeszukiwanie pamięci wymaga transformacji oryginalnego zapytania, w tym także generowania dodatkowych zapytań, także w oparciu o wspomniany przed chwilą kontekst. Wówczas proste wiadomość użytkownika: "Cześć, jak się masz?" może zostać zmieniona na serię pytań o aktualne otoczenie, samego użytkownika czy personę agenta. Modele językowe są w stanie bardzo skutecznie generować takie zapytania, eksplorując tym samym posiadane informacje. 
- **Cache Promptu:** Najnowsze możliwości "cache'owania" promptu, połączone z dużym oknem kontekstu modeli Gemini, przekładają się na zupełnie nowe możliwości pracy z pamięcią. W większości przypadków, agent będzie mógł wczytać wszystkie dostępne informacje do jednego promptu, i wielokrotnie je odpytywać. Tutaj musimy tylko pamiętać o ograniczonej zdolności do utrzymania uwagi modelu, co może sprawić, że takie podejście nie sprawdzi się w każdym przypadku. Może to być jednak problem, który niebawem zostanie rozwiązany. 
- **Agent:** Podobnie jak w przykładzie na temat **rozumowania**, za obsługę pamięci może odpowiadać wyspecjalizowany w tym agent.

## Frameworki

Aktualnie na rynku pojawiają się frameworki oferujące możliwość prostego tworzenia agentów, takich jak: [CrewAI](https://www.crewai.com/), [LangGraph](https://langchain-ai.github.io/langgraph/), [Swarm](https://github.com/openai/swarm), [AutoGen](https://github.com/microsoft/autogen). Patrząc na nie w pierwszej chwili, można dojść do wniosku, że większość rzeczy, których nauczyliśmy się w AI_devs 3 nie była potrzebna, bo w pełni adresuje je framework. 

Poniżej widzimy prosty wariant agenta, który można porównać do przykładu `agent` z początku bieżącej lekcji. Zaledwie 28 linii kodu wystarczyło do skorzystania z OpenAI oraz FireCrawl, aby pobrać listę 3 wpisów ze strony głównej Hacker News. 

![](https://cloud.overment.com/2024-10-26/aidevs3_crew-3b751c2b-5.png)

Problemy zaczynają się jednak, gdy musimy dokładniej określić zadanie i dostosować zachowanie agenta do naszych potrzeb. Przykłady, które omawialiśmy w poprzednich lekcjach, często pokazywały, jak ważne jest dbanie o szczegóły ukryte za warstwą abstrakcji.

Prawdopodobnie jeszcze większym problemem związanym z frameworkami są ich aktualizacje, które przy obecnym tempie rozwoju generatywnego AI często wymagają wprowadzania zmian w API. Przykładem jest przejście z "Function Call" na Tool Use, pojawienie się Real Time API od OpenAI czy Computer Use od Anthropic. W efekcie frameworki takie jak LangChain przeszły kilka transformacji, a wiele metod jest oznaczonych jako `deprecated`, co stanowi wyzwanie nawet w przypadku aplikacji budowanych 6-12 miesięcy temu.

Ostatecznie, prędzej czy później jeden z tych frameworków zdobędzie większą popularność i ustabilizuje się na tyle, że będzie można z powodzeniem stosować go w produkcyjnych projektach. Do tego czasu, znacznie lepiej jest oprzeć swoje projekty o własne rozwiązania. No chyba, że zależy nam wyłącznie na zbudowaniu szybkiego prototypu czy PoC. W każdym razie, warto obserwować rozwój frameworków, ponieważ obecnie stanowią świetne źródło inspiracji, a także momentami mogą okazać się przydatne w pojedynczych komponentach naszych agentów. 

## Podsumowanie

Patrząc na koncepcję agenta, mówimy tutaj o logice zamkniętej w serii pętli, kontrolowanych przez model językowy, z dodatkowymi ograniczeniami po stronie programistycznej. 

Obecnie ze względu na ograniczenia modeli językowych, raczej będzie zależało nam na tworzeniu **wyspecjalizowanych** agentów, posiadających kilka-kilkanaście umiejętności oraz stosunkowo prostą pamięć. 

Mimo to, główne elementy logiki powinny być projektowane uniwersalnie. Dzięki temu, gdy dodamy do agenta nowe narzędzie, nie będzie potrzeby wprowadzania dodatkowych zmian, aby móc z niego korzystać. 

Dobrym pomysłem jest także zbudowanie własnego zestawu narzędzi, które będziemy mogli podłączać do różnych agentów. Część z nich zbudowaliśmy już w dotychczasowych lekcjach AI_devs 3 i stanowią one dobry punkt startowy. Ostatecznie jednak, warto pomyśleć o własnej bibliotece.

No i na sam koniec dodam, że wskazana jest tutaj duża kreatywność. Choć pojawiają się schematy, wzorce i dobre praktyki budowania agentów, warto je poznać, a potem tworzyć własne.

Jeśli z tej lekcji masz zabrać ze sobą tylko jedną rzecz, to nie będzie to zbudowanie własnego, prostego agenta, na wzór tego z przykładu `agent`. Spróbuj połączyć go z jednym lub dwoma narzędziami i spraw, aby przeszedł poprawnie 10 Twoich testów. 

Powodzenia!